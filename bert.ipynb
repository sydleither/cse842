{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ga8Urj-kVL"
      },
      "source": [
        "Running multiclass text classification using BERT. I will switch from running in colab to a server as the project continues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbWzmo_mpX5b",
        "outputId": "ab09af7c-3818-4d54-a5dd-91a83308c677"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR-1Fh0DpOEX"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KFgUU-rvWT6F",
        "outputId": "bf99708a-ba48-4c45-d31e-8606a1c01fed"
      },
      "source": [
        "#BERT implementation based on https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb\n",
        "\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "MODEL = 'bert-base-uncased'\n",
        "BATCH_SIZE = 16\n",
        "args = TrainingArguments(\n",
        "    f\"{MODEL}-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='logs'\n",
        ")\n",
        "\n",
        "\n",
        "class Data(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "\n",
        "def train_test_val(text):\n",
        "    cat_map = {'Unbalanced_power_relations':0, 'Shallow_solution':1, \n",
        "               'Presupposition':2, 'Authority_voice':3, 'Metaphors':4,\n",
        "               'Compassion':5, 'The_poorer_the_merrier':6}\n",
        "    \n",
        "    data = text.split('\\n')[4:-1]\n",
        "    shuffle(data)\n",
        "    \n",
        "    X = []\n",
        "    y = []\n",
        "    for line in data:\n",
        "        columns = line.split('\\t')\n",
        "        X.append(columns[-3]) #using the 'span' of PCL to train\n",
        "        y.append(cat_map[columns[-2]])\n",
        "\n",
        "    X_train, X_test, X_val = np.split(X, [int(.6*len(X)), int(.8*len(X))])\n",
        "    y_train, y_test, y_val = np.split(y, [int(.6*len(y)), int(.8*len(y))])\n",
        "\n",
        "    return X_train.tolist(), y_train.tolist(), X_test.tolist(), \\\n",
        "           y_test.tolist(), X_val.tolist(), y_val.tolist()\n",
        "\n",
        "\n",
        "pcl = open('/content/drive/My Drive/data/dontpatronizeme_categories.tsv').read() #TODO some sentences have <h>, not sure if mistake\n",
        "X_train, y_train, X_test, y_test, X_val, y_val = train_test_val(pcl)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "encoded_train = tokenizer(X_train)\n",
        "encoded_test = tokenizer(X_test)\n",
        "encoded_val = tokenizer(X_val)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=7)\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=Data(encoded_train, y_train),\n",
        "    eval_dataset=Data(encoded_test, y_test),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "trainer.train()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 1675\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 525\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [525/525 03:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.296107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.164997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.149105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.171006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.034100</td>\n",
              "      <td>1.171144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 558\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to bert-base-uncased-finetuned/checkpoint-105\n",
            "Configuration saved in bert-base-uncased-finetuned/checkpoint-105/config.json\n",
            "Model weights saved in bert-base-uncased-finetuned/checkpoint-105/pytorch_model.bin\n",
            "tokenizer config file saved in bert-base-uncased-finetuned/checkpoint-105/tokenizer_config.json\n",
            "Special tokens file saved in bert-base-uncased-finetuned/checkpoint-105/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 558\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to bert-base-uncased-finetuned/checkpoint-210\n",
            "Configuration saved in bert-base-uncased-finetuned/checkpoint-210/config.json\n",
            "Model weights saved in bert-base-uncased-finetuned/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in bert-base-uncased-finetuned/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in bert-base-uncased-finetuned/checkpoint-210/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 558\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to bert-base-uncased-finetuned/checkpoint-315\n",
            "Configuration saved in bert-base-uncased-finetuned/checkpoint-315/config.json\n",
            "Model weights saved in bert-base-uncased-finetuned/checkpoint-315/pytorch_model.bin\n",
            "tokenizer config file saved in bert-base-uncased-finetuned/checkpoint-315/tokenizer_config.json\n",
            "Special tokens file saved in bert-base-uncased-finetuned/checkpoint-315/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 558\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to bert-base-uncased-finetuned/checkpoint-420\n",
            "Configuration saved in bert-base-uncased-finetuned/checkpoint-420/config.json\n",
            "Model weights saved in bert-base-uncased-finetuned/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in bert-base-uncased-finetuned/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in bert-base-uncased-finetuned/checkpoint-420/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 558\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to bert-base-uncased-finetuned/checkpoint-525\n",
            "Configuration saved in bert-base-uncased-finetuned/checkpoint-525/config.json\n",
            "Model weights saved in bert-base-uncased-finetuned/checkpoint-525/pytorch_model.bin\n",
            "tokenizer config file saved in bert-base-uncased-finetuned/checkpoint-525/tokenizer_config.json\n",
            "Special tokens file saved in bert-base-uncased-finetuned/checkpoint-525/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=525, training_loss=1.0118289130074638, metrics={'train_runtime': 222.5358, 'train_samples_per_second': 37.634, 'train_steps_per_second': 2.359, 'total_flos': 206504170861800.0, 'train_loss': 1.0118289130074638, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "DAi9o8wj2U68",
        "outputId": "ea389e78-87df-43f0-dc81-b8abc39e5989"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "outputs = trainer.predict(Data(encoded_val, y_val))\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "print(outputs.metrics)\n",
        "print(precision_score(y_val, y_pred, average=None, zero_division=1)) #last class has no samples represented as it's too small"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 559\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'test_loss': 1.239670991897583, 'test_runtime': 3.9807, 'test_samples_per_second': 140.426, 'test_steps_per_second': 8.792}\n",
            "[0.62184874 0.59459459 0.30612245 0.29166667 0.73529412 0.53107345\n",
            " 1.        ]\n"
          ]
        }
      ]
    }
  ]
}